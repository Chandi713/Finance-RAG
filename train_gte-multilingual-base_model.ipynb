{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050440ec-b784-46bc-a8ad-2a75cd11f741",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9f9531-6297-45dd-a43b-81443afad308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Capstone/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset # Kept for user's potential future use\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, SentenceTransformerModelCardData\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "import os\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from peft import LoraConfig, TaskType, get_peft_model # Ensure LoraConfig and TaskType are imported\n",
    "from sklearn.metrics.pairwise import cosine_similarity # For inference test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633b9be-7913-4728-b683-9aa8c8e29758",
   "metadata": {},
   "source": [
    "# 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63bee73-0179-4cad-90cf-0760ffe41d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "new_model_name = \"gte-finance-model\" # Path for the final fine-tuned model\n",
    "train_csv_file = \"train_df.csv\"\n",
    "eval_csv_file = \"eval_df.csv\"\n",
    "train_df = load_dataset(\"csv\", data_files=\"train_df.csv\")\n",
    "eval_df = load_dataset(\"csv\", data_files=\"eval_df.csv\")\n",
    "\n",
    "\n",
    "# --- LoRA Parameters ---\n",
    "lora_r = 16          # LoRA attention dimension (rank)\n",
    "lora_alpha = 32      # Alpha parameter for LoRA scaling\n",
    "lora_dropout = 0.1   # Dropout probability for LoRA layers\n",
    "lora_target_modules = [\"qkv_proj\", \"o_proj\", \"up_gate_proj\", \"down_proj\"]\n",
    "\n",
    "# --- Training Parameters ---\n",
    "training_output_dir = f\"{new_model_name}_training_checkpoints\" # Directory for checkpoints\n",
    "num_train_epochs = 4    # Adjust as needed\n",
    "train_batch_size = 8    # Adjust based on your VRAM\n",
    "eval_batch_size = 16    # Adjust based on your VRAM\n",
    "learning_rate = 2e-5\n",
    "warmup_ratio = 0.1      # Percentage of training steps for warmup\n",
    "model_max_length = 512  # Max sequence length for GTE models\n",
    "\n",
    "print(\"=== LoRA Fine-tuning for Sentence Embedding Models (from CSV - No Error Handling) ===\")\n",
    "print(f\"Base model: {base_model_id}\")\n",
    "print(f\"Output model name: {new_model_name}\")\n",
    "print(f\"Training data CSV: {train_csv_file}\")\n",
    "print(f\"Evaluation data CSV: {eval_csv_file}\")\n",
    "print(f\"LoRA r: {lora_r}, LoRA alpha: {lora_alpha}, LoRA dropout: {lora_dropout}\")\n",
    "print(f\"LoRA Target modules: {lora_target_modules} (CRITICAL: Verify these!)\")\n",
    "print(f\"Max sequence length: {model_max_length}\")\n",
    "print(\"=================================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac15f5-b35f-4a56-bd14-bd9988301cf0",
   "metadata": {},
   "source": [
    "# 2. Load and Prepare Dataset from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036dce9-94bb-4805-bc5f-b19b02df3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_examples_for_evaluator(dataset, stage=\"evaluator_data_prep\"):\n",
    "    \"\"\"\n",
    "    Prepares a list of InputExample objects from a Hugging Face Dataset.\n",
    "    This is now primarily used for setting up the EmbeddingSimilarityEvaluator.\n",
    "    \"\"\"\n",
    "    input_examples = []\n",
    "    if dataset is None or len(dataset) == 0:\n",
    "        print(f\"No data provided or dataset is empty for {stage}.\")\n",
    "        return input_examples\n",
    "\n",
    "    sample_item = dataset[0]\n",
    "    query_col_name = 'query'\n",
    "    corpus_col_name = 'corpus'\n",
    "\n",
    "    if query_col_name not in sample_item:\n",
    "        print(f\"Warning: Column '{query_col_name}' not found in {stage} data sample. Ensure your CSV has this column. Current columns: {list(sample_item.keys())}\")\n",
    "    if corpus_col_name not in sample_item:\n",
    "         print(f\"Warning: Column '{corpus_col_name}' not found in {stage} data sample. Ensure your CSV has this column. Current columns: {list(sample_item.keys())}\")\n",
    "\n",
    "    for item_index, item in enumerate(dataset):\n",
    "        query_text = item.get(query_col_name) # Use .get() for safety if columns might be missing in some rows\n",
    "        corpus_text = item.get(corpus_col_name)\n",
    "\n",
    "        if query_text is None or corpus_text is None:\n",
    "            print(f\"Warning: Skipping item at index {item_index} in {stage} due to missing '{query_col_name}' or '{corpus_col_name}'.\")\n",
    "            continue\n",
    "        \n",
    "        if not isinstance(query_text, str) or not isinstance(corpus_text, str):\n",
    "            print(f\"Warning: Item at index {item_index} in {stage} data has non-string content. Query: '{str(query_text)[:50]}...', Corpus: '{str(corpus_text)[:50]}...'. Attempting to convert to string.\")\n",
    "            query_text = str(query_text)\n",
    "            corpus_text = str(corpus_text)\n",
    "\n",
    "        example = InputExample(texts=[query_text, corpus_text], label=1.0)\n",
    "        input_examples.append(example)\n",
    "\n",
    "    print(f\"Created {len(input_examples)} InputExamples for {stage} from {len(dataset)} raw items.\")\n",
    "    return input_examples\n",
    "\n",
    "print(\"Loading and preparing dataset from CSV files...\")\n",
    "\n",
    "# Load training data - script will fail here if file not found or format is incorrect\n",
    "train_dataset_dict = load_dataset(\"csv\", data_files=train_csv_file)\n",
    "train_data_from_csv = train_dataset_dict['train'] # This is a Hugging Face Dataset object\n",
    "print(f\"Successfully loaded training data from {train_csv_file}. Number of examples: {len(train_data_from_csv)}\")\n",
    "if not train_data_from_csv or len(train_data_from_csv) == 0: # Check if the loaded HF dataset is empty\n",
    "    raise ValueError(f\"Training data loaded from {train_csv_file} is empty. Ensure the file contains data and has 'query' and 'corpus' columns.\")\n",
    "\n",
    "# Load evaluation data - script will fail here if file not found or format is incorrect\n",
    "eval_dataset_dict = load_dataset(\"csv\", data_files=eval_csv_file)\n",
    "eval_data_from_csv = eval_dataset_dict['train'] # This is a Hugging Face Dataset object\n",
    "print(f\"Successfully loaded evaluation data from {eval_csv_file}. Number of examples: {len(eval_data_from_csv)}\")\n",
    "if not eval_data_from_csv or len(eval_data_from_csv) == 0: # Check if the loaded HF dataset is empty\n",
    "    print(f\"Warning: Evaluation data loaded from {eval_csv_file} is empty. Trainer evaluation might be skipped or may cause issues.\")\n",
    "    # eval_data_from_csv will be an empty Dataset object, trainer should handle this.\n",
    "\n",
    "# Prepare data specifically for the EmbeddingSimilarityEvaluator\n",
    "eval_input_examples_for_custom_evaluator = prepare_input_examples_for_evaluator(eval_data_from_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a70f24-7350-4797-b18c-8c437c13840a",
   "metadata": {},
   "source": [
    "# 3. Initialize Model, Add LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094f098-6871-4144-b568-634aa27c4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nInitializing Sentence Transformer model: {base_model_id}\")\n",
    "model = SentenceTransformer(\n",
    "    model_name_or_path=base_model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.max_seq_length = model_max_length\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    target_modules=lora_target_modules,\n",
    ")\n",
    "\n",
    "model.add_adapter(peft_config)\n",
    "print(\"LoRA adapter added to the SentenceTransformer model (or underlying transformer).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a30b5c1-0319-47f1-b860-c6cfa0c49c9c",
   "metadata": {},
   "source": [
    "# 4. Setup Training Arguments, Loss, Evaluator, and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afa21d-6f70-4126-94e9-bae98403442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSetting up training components...\")\n",
    "\n",
    "# Determine evaluation strategy based on the loaded Hugging Face eval dataset\n",
    "perform_evaluation = bool(eval_data_from_csv and len(eval_data_from_csv) > 0)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=training_output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size, # This will be used if evaluation is performed\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    run_name=f\"{os.path.basename(base_model_id)}-lora-finetune-csv-noerr-{lora_r}\",\n",
    "    # Removed other arguments as per user request.\n",
    "    # Default behaviors for logging, saving, evaluation strategy will apply.\n",
    "    # For example, evaluation_strategy defaults to \"no\" if eval_dataset is not provided to Trainer.\n",
    "    # Logging defaults to \"steps\" with logging_steps=500.\n",
    "    # Saving defaults to \"steps\" with save_steps=500.\n",
    ")\n",
    "\n",
    "train_loss = MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "evaluator = None\n",
    "if eval_input_examples_for_custom_evaluator: # Check the list prepared for the custom evaluator\n",
    "    print(f\"Setting up EmbeddingSimilarityEvaluator with {len(eval_input_examples_for_custom_evaluator)} examples.\")\n",
    "    eval_queries = [example.texts[0] for example in eval_input_examples_for_custom_evaluator]\n",
    "    eval_corpus_texts = [example.texts[1] for example in eval_input_examples_for_custom_evaluator]\n",
    "    eval_scores = [1.0 for _ in eval_input_examples_for_custom_evaluator] # Assuming positive pairs\n",
    "\n",
    "    evaluator = EmbeddingSimilarityEvaluator(\n",
    "        sentences1=eval_queries,\n",
    "        sentences2=eval_corpus_texts,\n",
    "        scores=eval_scores,\n",
    "        main_similarity=None, # Defaults to cosine similarity\n",
    "        name=\"financial_qa_csv_custom_eval\", # Unique name for the evaluator\n",
    "        show_progress_bar=False,\n",
    "        write_csv=True\n",
    "    )\n",
    "else:\n",
    "    print(\"No data for EmbeddingSimilarityEvaluator or eval_data_from_csv was empty. Skipping custom evaluator setup.\")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_data_from_csv, # Pass the Hugging Face Dataset object\n",
    "    eval_dataset=eval_data_from_csv if perform_evaluation else None, # Pass the Hugging Face Dataset object\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator, # Pass the custom evaluator if created\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401b570-cf65-4650-86a2-7d759575480d",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476404c-aaf1-4b60-ab88-5690438b8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b39adc-6284-4dfb-a8ad-ef34018247dd",
   "metadata": {},
   "source": [
    "# 6. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f82f6b-57a5-4468-93e2-2f2a3ffc7f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = new_model_name\n",
    "print(f\"\\nSaving fine-tuned SentenceTransformer model to: {final_model_path}\")\n",
    "os.makedirs(final_model_path, exist_ok=True)\n",
    "model.save(\n",
    "    path=final_model_path,\n",
    "    model_name=os.path.basename(final_model_path),\n",
    ")\n",
    "print(f\"Model saved to {final_model_path}\")\n",
    "print(f\"You can load this model using: SentenceTransformer('{final_model_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c23f15-e7b0-4a9b-ac27-7b9111c574ba",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b811b63c-38f3-4c6a-8dbd-701c3a3f1da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/anaconda3/envs/Capstone/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/opt/anaconda3/envs/Capstone/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: PeftModelForFeatureExtraction \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('gte-finance-model', trust_remote_code=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13130708-f017-4315-bc2d-beec8be34fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6412da0-2afd-4434-9d06-6e31b1ca08ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 1024, 'do_lower_case': False}) with Transformer model: PeftModelForFeatureExtraction \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766aaa4-66f6-4e66-b0fe-0dd6f547f562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
